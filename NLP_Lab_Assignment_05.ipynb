{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BandiSreesaicharan/NLP/blob/main/NLP_Lab_Assignment_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QAdt-Ah5lSjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dd64d8-0ee7-490b-9166-3dac5b4f4c72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (first 1000 rows for safety)\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column\n",
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'processed_summaries']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r978js3jrHKo"
      },
      "outputs": [],
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ftycYW7Nrfn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4576942-df86-44e5-cdc8-27b67270f7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RF7Qm3x5opNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5d706d-a834-4f74-b008-1a5cfcc43912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                  filtered_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_summaries\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'filtered_summaries']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vqHN9fM2o5EB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ee4ef1-1d84-42e3-b11e-0ceaf3f06512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  filtered_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_summaries', 'lemmatized_summaries']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "11RmFPIdpJdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a26aa5a-e30a-4f4e-99ae-9612304e206a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     clean_summaries  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ],
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_summaries', 'clean_summaries']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yd-DBGkXpWqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9badb00-211f-4eb8-dfaf-12a5227741e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                            clean_summaries_pipeline  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # required in newer NLTK versions\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Step 1: Define preprocessing function (regex cleaning)\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep alphanumeric + spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Unified NLTK preprocessing pipeline\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    # Regex cleaning\n",
        "    cleaned = preprocess_text(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(cleaned)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "    # Rejoin words\n",
        "    final_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "# Step 3: Apply pipeline to dataset\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'clean_summaries_pipeline']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_iYdDOHudy4",
        "outputId": "4074dde5-e813-41b0-e429-2706e686d7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [Stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [The, recent, advancements, in, artificial, in...   \n",
            "2  [In, this, paper, ,, we, proposed, a, novel, m...   \n",
            "3  [Consistency, training, has, proven, to, be, a...   \n",
            "4  [To, ensure, safety, in, automated, driving, ,...   \n",
            "\n",
            "                                       pos_summaries  \n",
            "0  [(Stereo, NNP), (matching, NN), (is, VBZ), (on...  \n",
            "1  [(The, DT), (recent, JJ), (advancements, NNS),...  \n",
            "2  [(In, IN), (this, DT), (paper, NN), (,, ,), (w...  \n",
            "3  [(Consistency, NN), (training, NN), (has, VBZ)...  \n",
            "4  [(To, TO), (ensure, VB), (safety, NN), (in, IN...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # required in newer NLTK versions\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # required in newer NLTK versions\n",
        "\n",
        "# Load dataset (first 1000 rows for safety)\n",
        "# df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000) # Comment this line out to avoid overwriting df\n",
        "\n",
        "# Step 1: Tokenize summaries (if not already done)\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: Apply POS tagging to all tokenized summaries\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'pos_summaries']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ObA27s1W0snl"
      },
      "outputs": [],
      "source": [
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UK_6AkhJzRWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a2648e-385f-47cf-cfd6-0e9f7790cf41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'stereo': 5, 'matching': 3, 'image': 2, 'speed': 2, 'application': 2, 'segmentation': 2, 'network': 2, 'term': 2, 'technique': 1, 'depth': 1, 'topic': 1, 'research': 1, 'find': 1, 'navigation': 1, '3d': 1, 'reconstruction': 1, 'field': 1, 'correspondence': 1, 'area': 1, 'challenge': 1, 'development': 1, 'cue': 1, 'result': 1, 'architecture': 1, 'leverage': 1, 'advantage': 1, 'paper': 1, 'aim': 1, 'comparison': 1, 'state': 1, 'art': 1, 'accuracy': 1, 'importance': 1, 'realtime': 1})\n",
            "Verb Frequency: Counter({'used': 2, 'matching': 1, 'inferring': 1, 'owing': 1, 'become': 1, 'driving': 1, 'finding': 1, 'nontextured': 1, 'shown': 1, 'improve': 1, 'proposed': 1, 'give': 1})\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(df['clean_summaries_pipeline'][0])\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        " if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "  nouns.append(token.text)\n",
        " elif token.pos_ == \"VERB\":\n",
        "  verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i-2URZex0y0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727e8cb7-8d00-4087-da04-d2ec7d588aab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample named entities:\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "5  High-quality training data play a key role in ...   \n",
            "6  Semantic segmentation of fine-resolution urban...   \n",
            "7  To mitigate the radiologist's workload, comput...   \n",
            "8  Generalising deep models to new data from new ...   \n",
            "9  The success of deep learning methods in medica...   \n",
            "\n",
            "                                      named_entities  \n",
            "0                                    [(Stereo, GPE)]  \n",
            "1  [(AI, ORGANIZATION), (AI, ORGANIZATION), (Euro...  \n",
            "2                                                 []  \n",
            "3  [(Consistency, GSP), (Atrial Segmentation, ORG...  \n",
            "4  [(Gaussian Mixture Models, PERSON), (GMM, ORGA...  \n",
            "5                         [(EdgeFlow, ORGANIZATION)]  \n",
            "6  [(Semantic, GPE), (CNNs, ORGANIZATION), (Visio...  \n",
            "7                      [(AutoEncoder, ORGANIZATION)]  \n",
            "8                                  [(Hence, PERSON)]  \n",
            "9                                                 []  \n",
            "\n",
            "Top 10 most frequent named entities in sample:\n",
            "[('EHT', 4), ('Mask', 3), ('AI', 2), ('GMM', 2), ('Morphological Snakes', 2), ('CNNs', 2), ('Stereo', 1), ('European', 1), ('Health', 1), ('Robustness', 1)]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Reload dataset safely\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Step 1: Tokenize\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: POS tagging\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Step 3: NER\n",
        "def extract_entities(pos_tags):\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    entities = []\n",
        "    for subtree in chunked:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entities.append((entity, subtree.label()))\n",
        "    return entities\n",
        "\n",
        "# Apply to a small sample first to test\n",
        "df_sample = df.head(10).copy()\n",
        "df_sample['named_entities'] = df_sample['pos_summaries'].apply(extract_entities)\n",
        "\n",
        "print(\"Sample named entities:\")\n",
        "print(df_sample[['summaries', 'named_entities']])\n",
        "\n",
        "# Step 4: Frequency analysis (on sample for speed)\n",
        "all_entities = []\n",
        "for ents in df_sample['named_entities']:\n",
        "    all_entities.extend([entity for entity, label in ents])\n",
        "\n",
        "entity_freq = Counter(all_entities)\n",
        "print(\"\\nTop 10 most frequent named entities in sample:\")\n",
        "print(entity_freq.most_common(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q95f6VFfkWlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}